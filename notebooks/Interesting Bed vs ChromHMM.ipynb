{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T13:22:04.031028Z",
     "start_time": "2017-10-16T13:22:02.405011Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import subprocess \n",
    "import os\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from IPython.display import display\n",
    "import pybedtools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T13:22:05.542348Z",
     "start_time": "2017-10-16T13:22:05.532696Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results_dir = Path(\"/mnt/stripe/bio/experiments/aging/loci_of_interest.tables\")\n",
    "# sorted_root = Path(\"/mnt/stripe/bio/experiments/aging/loci.sorted\")\n",
    "# THREADS_N = 32\n",
    "\n",
    "results_dir = Path(\"/Volumes/BigData/bio/experiments/aging/loci_of_interest.tables\")\n",
    "sorted_root = Path(\"/Volumes/BigData/bio/experiments/aging/loci.sorted\")\n",
    "THREADS_N = 8\n",
    "\n",
    "results_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T18:27:58.981602Z",
     "start_time": "2017-10-11T18:27:58.958617Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pybedtools.set_tempdir(\"/tmp\")\n",
    "pybedtools.cleanup()\n",
    "# !rm {sorted_root}\n",
    "# !rm {results_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T13:22:09.045202Z",
     "start_time": "2017-10-16T13:22:09.037525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loci_root = Path(\"/mnt/stripe/bio/raw-data/aging/loci_of_interest\")\n",
    "# golden_peaks_root = Path(\"/mnt/stripe/bio/experiments/aging/peak_calling\")\n",
    "# zinbra_peaks_root = Path(\"/mnt/stripe/bio/experiments/configs/Y20O20/peaks\")\n",
    "\n",
    "loci_root = Path(\"/Volumes/BigData/bio/raw-data/aging/loci_of_interest\")\n",
    "golden_peaks_root = Path(\"/Volumes/BigData/bio/experiments/aging/peak_calling\") # *.*Peak\n",
    "zinbra_peaks_root = Path(\"/Volumes/BigData/bio/experiments/configs/Y20O20/peaks\") # *.bed\n",
    "\n",
    "diff_chip_root = \"/mnt/stripe/bio/raw-data/aging/chipseq_diff_loci\"\n",
    "diff_chip_root = loci_root / \"chipseq_diff_loci\"\n",
    "\n",
    "signal_root = Path(\"/mnt/stripe/bio/experiments/signal\")\n",
    "\n",
    "chromhmm_root = loci_root / \"chromhmm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T15:25:38.611729Z",
     "start_time": "2017-10-11T15:25:38.429550Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls {loci_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T15:26:33.101949Z",
     "start_time": "2017-10-11T15:26:32.619658Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls {zinbra_peaks_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T15:26:46.371854Z",
     "start_time": "2017-10-11T15:26:45.952399Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls {golden_peaks_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T17:56:12.585654Z",
     "start_time": "2017-10-13T17:56:12.449732Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls {diff_chip_root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T11:21:48.909575Z",
     "start_time": "2017-10-11T11:21:48.676768Z"
    }
   },
   "outputs": [],
   "source": [
    "chromhmm_paths = list(chromhmm_root.glob('*.bed'))\n",
    "chromhmm_paths.sort(key=lambda p: int(p.name.split(\".\")[2].split(\"_\")[0]))\n",
    "\n",
    "CHROMHMM_ST_MAP = {\n",
    "    \"1_TssA\": \"Active TSS\",\n",
    "    \"2_TssFlnk\": \"Flanking TSS\",\n",
    "    \"3_TssFlnkU\": \"Flanking TSS Upstream\",\n",
    "    \"4_TssFlnkD\": \"Flanking TSS Downstream\",\n",
    "    \"5_Tx\": \"Strong transcription\",\n",
    "    \"6_TxWk\": \"Weak transcription\",\n",
    "    \"7_EnhG1\": \"Genic enhancer1\",\n",
    "    \"8_EnhG2\": \"Genic enhancer2\",\n",
    "    \"9_EnhA1\": \"Active Enhancer 1\",\n",
    "    \"10_EnhA2\": \"Active Enhancer 2\",\n",
    "    \"11_EnhWk\": \"Weak Enhancer\",\n",
    "    \"12_ZNF_Rpts\": \"ZNF genes & repeats\",\n",
    "    \"13_Het\": \"Heterochromatin\",\n",
    "    \"14_TssBiv\": \"Bivalent/Poised TSS\",\n",
    "    \"15_EnhBiv\": \"Bivalent Enhancer\",\n",
    "    \"16_ReprPC\": \"Repressed PolyComb\",\n",
    "    \"17_ReprPCWk\": \"Weak Repressed PolyComb\",\n",
    "    \"18_Quies\": \"Quiescent/Low\",\n",
    "}\n",
    "\n",
    "def chromhmm_state_descr(s):\n",
    "    chunks = s.split(\".\")\n",
    "    if len(chunks) <= 2:\n",
    "        return s\n",
    "    return CHROMHMM_ST_MAP.get(chunks[2], s)\n",
    "\n",
    "for i, p in enumerate(chromhmm_paths):\n",
    "    print(chromhmm_state_descr(p.name), \"->\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot include all files from dir, because list is too big and heatmap becomes unreadable. Let's keep curated list\n",
    "of loci by rules:\n",
    "* root folder top level *.bed files\n",
    "* subfoldes: \"enchancers\", \"tfs\", \"regulatory\", \"weak_consensus\", \"zinbra_consensus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T15:50:55.286745Z",
     "start_time": "2017-10-11T15:50:55.273079Z"
    }
   },
   "outputs": [],
   "source": [
    "loci_paths = [p for p in loci_root.glob('*.bed')]\n",
    "for folder in [\"enchancers\", \"tfs\", \"regulatory\", \"weak_consensus\", \"zinbra_consensus\"]:\n",
    "    loci_paths.extend([p for p in (loci_root / folder).glob('**/*.bed')])\n",
    "loci_paths = sorted(loci_paths)\n",
    "loci_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff-Chipseq loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T17:57:39.996446Z",
     "start_time": "2017-10-13T17:57:39.988854Z"
    }
   },
   "outputs": [],
   "source": [
    "diff_chip_paths = [p for p in diff_chip_root.glob('**/*.bed')]\n",
    "diff_chip_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T17:37:25.233635Z",
     "start_time": "2017-10-16T17:37:25.196945Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def donor_order_id(path):\n",
    "    chunks = path.name.split('_')\n",
    "    cands = list(filter(lambda s: len(s) > 2 and (s.startswith(\"OD\") or s.startswith(\"YD\")), chunks))\n",
    "    if len(cands) > 0:\n",
    "        donor_id = cands[0]\n",
    "        if donor_id[2] != \"S\":\n",
    "            return (donor_id[:2], int(donor_id[2:]))\n",
    "\n",
    "    return (path.name, 0)\n",
    "    \n",
    "\n",
    "def collect_peaks(peaks_roots):\n",
    "    result = {}\n",
    "    for peaks_root in [x for x in peaks_roots.iterdir() if x.is_dir()]:\n",
    "        print(\"Peaks:\", peaks_root)\n",
    "\n",
    "        peaks = list(chain(peaks_root.glob(\"**/*.bed\"), peaks_root.glob(\"**/*.*Peak\")))\n",
    "        # e.g. \n",
    "        # * OD_OD14_H3K27ac_hg19_1.0E-6_peaks.bed\n",
    "        # * OD8_k27ac_hg19_broad_peaks.broadPeak\n",
    "        # * zinbra_weak_consensus.bed\n",
    "        peaks.sort(key=donor_order_id)\n",
    "        print(len(peaks))    \n",
    "        print(*[str(p) for p in peaks], sep=\"\\n\")\n",
    "        result[peaks_root.name] = peaks\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T17:37:30.153949Z",
     "start_time": "2017-10-16T17:37:29.819912Z"
    }
   },
   "outputs": [],
   "source": [
    "golden_peaks_by_histmod = collect_peaks(golden_peaks_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T17:37:33.262895Z",
     "start_time": "2017-10-16T17:37:32.418065Z"
    }
   },
   "outputs": [],
   "source": [
    "zinbra_peaks_by_histmod = collect_peaks(zinbra_peaks_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T17:59:52.585120Z",
     "start_time": "2017-10-16T17:59:52.578863Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zinbra_conensus_paths = [p for p in (loci_root / \"zinbra_consensus\").glob('*.bed')]\n",
    "zinbra_conensus_paths\n",
    "\n",
    "# Alternative:\n",
    "# zinbra_peaks_by_histmod = collect_peaks(zinbra_peaks_root)\n",
    "# consensus_peaks = []\n",
    "# for mod, peaks in zinbra_peaks_by_histmod.items():\n",
    "#     consensus_peaks.extend([p for p in peaks if \"consensus\" in p.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T17:59:53.840549Z",
     "start_time": "2017-10-16T17:59:53.833825Z"
    }
   },
   "outputs": [],
   "source": [
    "golden_conensus_paths = [p for p in (loci_root / \"golden_consensus\").glob('*.bed')]\n",
    "golden_conensus_paths\n",
    "\n",
    "# Alternative:\n",
    "# golden_peaks_by_histmod = collect_peaks(golden_peaks_root)\n",
    "# consensus_peaks = []\n",
    "# for mod, peaks in golden_peaks_by_histmod.items():\n",
    "#     consensus_peaks.extend([p for p in peaks if \"consensus\" in p.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T11:21:51.010379Z",
     "start_time": "2017-10-11T11:21:51.006469Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_loci = loci_paths + chromhmm_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T11:21:51.506747Z",
     "start_time": "2017-10-11T11:21:51.387034Z"
    }
   },
   "outputs": [],
   "source": [
    "!which bedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T11:21:51.601830Z",
     "start_time": "2017-10-11T11:21:51.582324Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bedtrace.py\n",
    "def run(commands, stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE):\n",
    "    \"\"\"Launches pipe of commands given stdin and final stdout, stderr\"\"\"\n",
    "    processes = []\n",
    "    _stdin = stdin\n",
    "    for i, cmd in enumerate(commands):\n",
    "        if i < len(commands) - 1:\n",
    "            _stdout = subprocess.PIPE\n",
    "        else:\n",
    "            _stdout = stdout\n",
    "            \n",
    "        p = subprocess.Popen(cmd, stdin=_stdin, stdout=_stdout,\n",
    "                             stderr=stderr)\n",
    "        processes.append(p)\n",
    "        _stdin = p.stdout\n",
    "\n",
    "    for i in range(0, len(processes)):\n",
    "        if i < len(processes) - 1:\n",
    "            # Allow p1 to receive a SIGPIPE if p2 exits.\n",
    "            processes[i].stdout.close()\n",
    "        else:\n",
    "            return processes[i].communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T18:00:43.321516Z",
     "start_time": "2017-10-13T18:00:43.289424Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "def as_sorted(p: Path, root: Path, sorted_root: Path):\n",
    "    sorted_p = sorted_root / p.relative_to(root)\n",
    "    sorted_p = sorted_p.parent / (sorted_p.stem + \".sorted.bed\")\n",
    "\n",
    "    if not sorted_p.exists():\n",
    "        sorted_p.parent.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Do not resort file if already sorted:\n",
    "        stderr = run(([\"sort\", \"-c\", \"-k1,1\", \"-k2,2n\", str(p)],))[1]\n",
    "        is_sorted = (len(stderr) == 0)\n",
    "        \n",
    "        if not is_sorted:\n",
    "            print(\"Sorting: \", str(p))\n",
    "            # By some reason BedTool.sort() fails to sort cds.csv\n",
    "            # bt.sort().saveas(sorted_p)\n",
    "            #stderr = run(([\"sort\", \"-c\", \"-k1,1\", \"-k2,2n\", str(sorted_p)],))[1]\n",
    "            #assert len(stderr) == 0, \"Expected to be sorted: {}\\nError:\\n{}\".format(sorted_p, stderr)\n",
    "            with open(str(sorted_p), \"w\") as f:\n",
    "                run(([\"sort\", \"-k1,1\", \"-k2,2n\", str(p)],), stdout=f)\n",
    "            print(\"  [Done]\", str(sorted_p))\n",
    "        else:   \n",
    "            # just copy file\n",
    "            shutil.copyfile(str(p), str(sorted_p))\n",
    "        \n",
    "        \n",
    "    return sorted_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T16:31:19.981038Z",
     "start_time": "2017-10-11T16:31:19.942314Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def as_sorted_bedtool(p: Path, root: Path, sorted_root: Path):\n",
    "#     sorted_p = sorted_root / p.relative_to(root)\n",
    "#     sorted_p = sorted_p.parent / (sorted_p.stem + \".sorted.bed\")\n",
    "\n",
    "#     if not sorted_p.exists():\n",
    "#         sorted_p.parent.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "#         # Do not resort file if already sorted:\n",
    "#         stderr = run(([\"sort\", \"-c\", \"-k1,1\", \"-k2,2n\", str(p)],))[1]\n",
    "#         is_sorted = (len(stderr) == 0)\n",
    "        \n",
    "#         bt = pybedtools.bedtool.BedTool(str(p))\n",
    "#         if not is_sorted:\n",
    "#             print(\"Sorting: \", str(p))\n",
    "#             # By some reason BedTool.sort() fails to sort cds.csv\n",
    "#             # bt.sort().saveas(sorted_p)\n",
    "#             #stderr = run(([\"sort\", \"-c\", \"-k1,1\", \"-k2,2n\", str(sorted_p)],))[1]\n",
    "#             #assert len(stderr) == 0, \"Expected to be sorted: {}\\nError:\\n{}\".format(sorted_p, stderr)\n",
    "#             with open(str(sorted_p), \"w\") as f:\n",
    "#                 run(([\"sort\", \"-k1,1\", \"-k2,2n\", str(p)],), stdout=f)\n",
    "#             print(\"  [Done]\", str(sorted_p))\n",
    "#         else:   \n",
    "#             # just copy file\n",
    "#             bt.saveas(str(sorted_p))\n",
    "#         del bt  # Too many open files issue\n",
    "        \n",
    "#     return pybedtools.bedtool.BedTool(str(sorted_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T17:53:04.401113Z",
     "start_time": "2017-10-16T17:53:04.315400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, TimeoutError\n",
    "\n",
    "def run_bedtools_uniq_wc(ij, a, b):\n",
    "    output = run(([\"bedtools\", \"intersect\", \"-a\", str(a),\n",
    "                   \"-b\", str(b), \"-wa\"],\n",
    "                  [\"uniq\"], [\"wc\", \"-l\"]))\n",
    "    return (ij, int(output[0].decode().strip()))\n",
    "\n",
    "# def run_bedtools_uniq_wc(ij, a: pybedtools.BedTool, b: pybedtools.BedTool):\n",
    "#     # a = as_sorted_bedtool(a)\n",
    "#     # b = as_sorted_bedtool(b)\n",
    "#     c = a.intersect(b, wa=True)\n",
    "#     output = run(([\"cat\", c.fn], [\"uniq\"], [\"wc\", \"-l\"]))\n",
    "#     del c  # To many open files issues\n",
    "#     return (ij, int(output[0].decode().strip()))\n",
    "\n",
    "def run_bedtools_jaccard(ij, a, b):\n",
    "    output = run(([\"~/work/washu/bed/jaccard.sh\", str(a), str(b)]))\n",
    "    stdout = output[0].decode().strip()\n",
    "    return (ij, float(stdout))\n",
    "\n",
    "# def run_bedtools_jaccard(ij, a, b):\n",
    "#     #bed tools jaccard not symmetrix\n",
    "#     output = run(([\"bedtools\", \"jaccard\", \"-a\", str(a),\n",
    "#                    \"-b\", str(b)],\n",
    "#                   [\"cut\", \"-f\", \"3\"]))\n",
    "#     stdout = output[0].decode().strip()\n",
    "#     lines = stdout.split(\"\\n\")\n",
    "#     assert len(lines) == 2, lines\n",
    "#     assert lines[0] == \"jaccard\"\n",
    "#     return (ij, float(lines[1]))\n",
    "\n",
    "# def run_bedtools_jaccard(ij, a: pybedtools.BedTool, b: pybedtools.BedTool):\n",
    "#     # a = as_sorted_bedtool(a)\n",
    "#     # b = as_sorted_bedtool(b)\n",
    "#     return (ij, a.jaccard(b)[\"jaccard\"])\n",
    "\n",
    "def calc_intersection_table(a_paths, b_paths, path_to_sorted,\n",
    "                            threads=4, timeout_hours=10, jaccard=False):   \n",
    "    path_pairs = []\n",
    "    for i, a in enumerate(a_paths, 0):\n",
    "        for j, b in enumerate(b_paths, 1):\n",
    "            path_pairs.append(((i,j), path_to_sorted[a], path_to_sorted[b]))\n",
    "\n",
    "    metric = run_bedtools_jaccard if jaccard else run_bedtools_uniq_wc\n",
    "    pool = Pool(processes=threads) \n",
    "    multiple_results = [pool.apply_async(metric, \n",
    "                                         (ij, a, b)) for ij, a, b in path_pairs]\n",
    "    values = [res.get(timeout=3600*timeout_hours) for res in multiple_results]\n",
    "    \n",
    "    x = np.zeros((len(a_paths), 1 + len(b_paths)), np.float32)\n",
    "    for (i,j), value in values:\n",
    "        x[i, j] = value\n",
    "    \n",
    "    for i, a in enumerate(a_paths, 0):\n",
    "        output = run(([\"cat\", str(a)],[\"wc\", \"-l\"],))\n",
    "        x[i, 0] = int(output[0].decode().strip())\n",
    "               \n",
    "    df = pd.DataFrame(x,\n",
    "                      index=[f.name for f in a_paths],\n",
    "                      columns=[\"total\"] + [f.name for f in b_paths])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T18:11:56.562585Z",
     "start_time": "2017-10-12T18:11:56.517870Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(title, df, path=None, autoscale=False, label_fun=None, figsize=(10,10),\n",
    "                 col_cluster=False, row_cluster=False):\n",
    "    if autoscale:\n",
    "        vmin, vmax = None, None\n",
    "    else:\n",
    "        vmin, vmax = 0, 1\n",
    "        \n",
    "    if label_fun:\n",
    "        df = df.copy()\n",
    "        df.columns = [label_fun(s) for s in df.columns]\n",
    "        df.index = [label_fun(s) for s in df.index]\n",
    "        \n",
    "    g = sns.clustermap(df,\n",
    "                       col_cluster=col_cluster, row_cluster=row_cluster,\n",
    "                       figsize=figsize, cmap=\"rainbow\",\n",
    "                       metric=\"chebyshev\",\n",
    "                       vmin=vmin, vmax=vmax, robust=True) #robust=True: ignore color outliers\n",
    "    plt.setp(g.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "\n",
    "    plt.title(title)\n",
    "    if path is None:        \n",
    "        plt.show()\n",
    "    else:\n",
    "        pp.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T17:23:04.686372Z",
     "start_time": "2017-10-11T17:23:04.671821Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_intersection_table(beds, loci, path_to_bt, result_path, threads=4, jaccard=False):\n",
    "    if result_path.exists():\n",
    "        df = pd.DataFrame.from_csv(result_path)\n",
    "        print(\"Loaded: \", result_path)\n",
    "    else:\n",
    "        print(\"Calculating: \", result_path)\n",
    "        df = calc_intersection_table(beds, loci, path_to_bt, threads=threads, jaccard=jaccard) \n",
    "        result_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(str(result_path))\n",
    "        print(\"  Saved: \", result_path)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T11:21:53.821142Z",
     "start_time": "2017-10-11T11:21:53.817511Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    return df.divide(df[\"total\"], axis=0).drop(\"total\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T17:42:11.832699Z",
     "start_time": "2017-10-11T17:42:11.799377Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_intersection(beds, loci, path_to_bt, results_dir, tag,\n",
    "                         figsize=(10,10), col_cluster=False, row_cluster=True):\n",
    "    df_bl = load_intersection_table(beds, loci, path_to_bt, \n",
    "                                    results_dir / \"{}_bl.csv\".format(tag), threads=THREADS_N)\n",
    "    display(df_bl.head(3))\n",
    "    \n",
    "    df_lb = load_intersection_table(loci, beds, path_to_bt,\n",
    "                                    results_dir / \"{}_lb.csv\".format(tag), threads=THREADS_N)\n",
    "    display(df_lb.head(3))\n",
    "    \n",
    "    df_n_bl = normalize(df_bl)\n",
    "    display(df_n_bl.head(3))\n",
    "\n",
    "    df_n_lb = normalize(df_lb).T\n",
    "    display(df_n_lb.head(3))\n",
    "\n",
    "    df_jaccard = load_intersection_table(beds, loci, path_to_bt, \n",
    "                                         results_dir / \"{}_js.csv\".format(tag), threads=THREADS_N,\n",
    "                                         jaccard = True)\n",
    "    df_jaccard = df_jaccard.drop(\"total\", axis=1)\n",
    "    display(df_jaccard.head(3))\n",
    "    \n",
    "    plot_heatmap(\"Metrics: # intervals from row file intersecting any interval from column file\",\n",
    "                 df_n_bl, autoscale=False, label_fun=chromhmm_state_descr, figsize=figsize,\n",
    "                 col_cluster=col_cluster, row_cluster=row_cluster)\n",
    "    plot_heatmap(\"Metrics: # intervals from col file intersecting any interval from row file\",\n",
    "                 df_n_lb, autoscale=False, label_fun=chromhmm_state_descr, figsize=figsize,\n",
    "                 col_cluster=col_cluster, row_cluster=row_cluster)\n",
    "    plot_heatmap(\"Metrics: Geometric mean for intersectiong intervals\",\n",
    "                 np.sqrt(df_n_bl*df_n_lb), autoscale=False, label_fun=chromhmm_state_descr, figsize=figsize,\n",
    "                 col_cluster=col_cluster, row_cluster=row_cluster)\n",
    "    plot_heatmap(\"Metrics: Jaccard\",\n",
    "                 df_jaccard, autoscale=True, label_fun=chromhmm_state_descr, figsize=figsize,\n",
    "                 col_cluster=col_cluster, row_cluster=row_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T09:24:34.365779Z",
     "start_time": "2017-10-12T09:24:26.806598Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "tmp_loci_paths = loci_paths[0:6]\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in tmp_loci_paths}\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(tmp_loci_paths, tmp_loci_paths, mapping, results_dir, \"tmp0_loci.csv\", figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T14:31:55.944678Z",
     "start_time": "2017-10-10T14:31:55.941977Z"
    }
   },
   "source": [
    "# Loci vs Loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T09:46:14.396321Z",
     "start_time": "2017-10-12T09:25:33.123699Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in loci_paths}\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(loci_paths, loci_paths, mapping, results_dir, \"loci\", figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loci vs ChromHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T09:19:08.431930Z",
     "start_time": "2017-10-12T09:19:08.407935Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in loci_paths}\n",
    "mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in chromhmm_paths})\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(loci_paths, chromhmm_paths, mapping, results_dir, \"loci_chromhmm\", figsize=(8, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T14:32:34.287847Z",
     "start_time": "2017-10-10T14:32:34.284244Z"
    }
   },
   "source": [
    "# ChromHMM vs ChromHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:19:47.588477Z",
     "start_time": "2017-10-11T12:19:45.275497Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in chromhmm_paths}\n",
    "print(\"[Done]\")\n",
    "process_intersection(chromhmm_paths, chromhmm_paths, mapping, results_dir, \"chromhmm\", figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff chipseq vs Loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T18:02:00.262595Z",
     "start_time": "2017-10-13T18:01:11.055252Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in diff_chip_paths}\n",
    "mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in chromhmm_paths})\n",
    "print(\"[Done]\")\n",
    "process_intersection(diff_chip_paths, chromhmm_paths, mapping, results_dir, \"diff_chip_chromhmm\", figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T18:07:18.976172Z",
     "start_time": "2017-10-13T18:02:00.264993Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in diff_chip_paths}\n",
    "mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in loci_paths})\n",
    "print(\"[Done]\")\n",
    "process_intersection(diff_chip_paths, loci_paths, mapping, results_dir, \"diff_chip_loci\", figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T18:36:58.342812Z",
     "start_time": "2017-10-13T18:36:53.227926Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zinbra_YO_consensus_paths = list((loci_root / \"zinbra_YO_consensus\").glob(\"*.bed\"))\n",
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in diff_chip_paths}\n",
    "mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in zinbra_YO_consensus_paths})\n",
    "print(\"[Done]\")\n",
    "process_intersection(diff_chip_paths, zinbra_YO_consensus_paths, mapping, results_dir, \"diff_chip_zinbra_YO_consensus\",\n",
    "                     figsize=(8,8), row_cluster=False, col_cluster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Consensus vs Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for p in (zinbra_conensus_paths + golden_conensus_paths):\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root)\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(consensus_peaks, all_loci, mapping, results_dir, \"consensus\", figsize=(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zinbra vs Loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:21:35.023637Z",
     "start_time": "2017-10-11T12:21:25.273821Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for p in zinbra_conensus_paths:\n",
    "    mapping[p] = as_sorted(p, zinbra_peaks_root, sorted_root)\n",
    "for p in all_loci:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(consensus_peaks, all_loci, mapping, results_dir, \"zinbra_consensus_vs_loci\", figsize=(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Hist mods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:20:55.221079Z",
     "start_time": "2017-10-11T12:20:45.915527Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for mod, peaks in zinbra_peaks_by_histmod.items():\n",
    "    for p in peaks:\n",
    "        mapping[p] = as_sorted(p, zinbra_peaks_root, sorted_root / \"zinbra\")\n",
    "for p in all_loci:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "for mod, peaks in zinbra_peaks_by_histmod.items():\n",
    "    process_intersection(peaks, all_loci, mapping, results_dir, \"zinbra_{}_vs_loci\".format(mod), figsize=(17,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macs vs Loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conensus peaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for p in golden_conensus_paths:\n",
    "    mapping[p] = as_sorted(p, golden_peaks_root, sorted_root)\n",
    "for p in all_loci:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(consensus_peaks, all_loci, mapping, results_dir, \"golden_consensus_vs_loci\", figsize=(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Hist mods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:23:09.460510Z",
     "start_time": "2017-10-11T12:22:57.250421Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for mod, peaks in golden_peaks_by_histmod.items():\n",
    "    for p in peaks:\n",
    "        mapping[p] = as_sorted(p, golden_peaks_root, sorted_root / \"golden\")\n",
    "for p in all_loci:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "for mod, peaks in golden_peaks_by_histmod.items():\n",
    "    process_intersection(peaks, all_loci, mapping, results_dir, \"golden_{}_vs_loci\".format(mod), figsize=(17,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: RNA-diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loci_root / \"rna_diff\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loci_root / \"pathway\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal (coverage) vs loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T18:23:21.580450Z",
     "start_time": "2017-10-12T18:23:21.575724Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:14:17.993371Z",
     "start_time": "2017-10-13T16:11:47.971641Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_dfs_by_datatype = {}\n",
    "signal_dfs_by_loci = {}\n",
    "\n",
    "series_by_loci = defaultdict(list)\n",
    "data_type_paths = [p for p in signal_root.iterdir() if p.is_dir()]\n",
    "for i, data_type_path in enumerate(data_type_paths, 1):\n",
    "    data_type = data_type_path.name\n",
    "    print(\"[{}/{}] Processing: {}\".format(i, len(data_type_paths), data_type))\n",
    "    \n",
    "    for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "        print(\"  Normalization:\", norm)\n",
    "        series_by_datatype = []\n",
    "        \n",
    "        # TODO: load from results dir?\n",
    "        for loci_path in (p for p in data_type_path.iterdir() if p.is_dir()):\n",
    "            loci = loci_path.name\n",
    "            files = [p for p in loci_path.glob(\"**/*_{}_data.csv\".format(norm))]\n",
    "            \n",
    "            assert len(files) <= 1, \"{}@{} [{}] Expected one file, but was {}: {}\".format(\n",
    "                data_type, loci, norm, len(files), files\n",
    "            )\n",
    "            if not len(files):\n",
    "                continue\n",
    "            \n",
    "            df = pd.DataFrame.from_csv(files[0] , header=None)\n",
    "            series = df.iloc[:,0]\n",
    "            series.name = loci\n",
    "            \n",
    "            series_by_datatype.append(series) \n",
    "            \n",
    "            series2 = series.copy()\n",
    "            series2.name = data_type\n",
    "            series_by_loci[(loci, norm)].append(series2)\n",
    "            \n",
    "        # by data type:    \n",
    "        df = pd.DataFrame(series_by_datatype, )\n",
    "        #df.index = [f.stem for f in itertools.islice(files, 10)]\n",
    "        df.to_csv(str(results_dir / \"signal_{}_{}\".format(data_type, norm)))\n",
    "        signal_dfs_by_datatype[(data_type, norm)] = df\n",
    "\n",
    "for (loci, norm), series in series_by_loci.items():\n",
    "    df = pd.DataFrame(series, )\n",
    "    df.to_csv(str(results_dir / \"signal_{}_{}\".format(loci, norm)))\n",
    "    signal_dfs_by_loci[(loci, norm)] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:08:46.867082Z",
     "start_time": "2017-10-13T16:08:46.840964Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_dfs_by_datatype[(\"H3K4me1\", \"rpkm\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:09:54.202948Z",
     "start_time": "2017-10-13T16:09:54.177877Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_dfs_by_loci[(\"washu_german_rrbs_filtered_dmrs_all_10.hg19\", \"rpkm\")].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_donors_heatmap(title, df, path=None, autoscale=False, \n",
    "                        label_fun=None, figsize=(10,10),\n",
    "                        donors_difference=True,\n",
    "                        col_cluster=False, row_cluster=False):\n",
    "    if autoscale:\n",
    "        vmin, vmax = None, None\n",
    "    else:\n",
    "        vmin, vmax = 0, 1\n",
    "        \n",
    "    if label_fun:\n",
    "        df = df.copy()\n",
    "        df.columns = [label_fun(s) for s in df.columns]\n",
    "        df.index = [label_fun(s) for s in df.index]\n",
    "        \n",
    "    donors_colors = [\"g\" if d.lower().startswith(\"od\") else (\"b\" if d.lower().startswith(\"YD\") else \"b\")\n",
    "                     for d in df.index]\n",
    "    row_colors = pd.Series(data=donors_colors, index=df.index, name=\"age\")\n",
    "            \n",
    "    g = sns.clustermap(df,\n",
    "                       col_cluster=col_cluster, row_cluster=row_cluster,\n",
    "                       figsize=figsize, cmap=\"rainbow\",\n",
    "                       metric=\"chebyshev\",\n",
    "                       standard_scale = 1 if donors_difference else 0,  #0 (rows) or 1 (columns)\n",
    "                       vmin=vmin, vmax=vmax,\n",
    "                       row_colors=row_colors,\n",
    "                       robust=True) #robust=True: ignore color outliers\n",
    "    plt.setp(g.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "\n",
    "    plt.title(title)\n",
    "    if path is None:        \n",
    "        plt.show()\n",
    "    else:\n",
    "        pp.savefig()\n",
    "        \n",
    "def plot_signal_heatmap(tag, metric, signal_dfs, *args,\n",
    "                        col_filter_fun=None,\n",
    "                        **kw):\n",
    "    df = signal_dfs[(tag, metric)].T\n",
    "        \n",
    "    # let's sort by index, not just lexicographically, but in human readable order, e.g. OD2 shoud be before OD10\n",
    "    def inner_donor_order_id(name):\n",
    "        assert (len(name) > 2 and (name.startswith(\"od\") or name.startswith(\"yd\")))\n",
    "        return (name[:2], int(name[2:]))\n",
    "\n",
    "    df = df.loc[sorted(df.index.tolist(), key=inner_donor_order_id), :]\n",
    "    \n",
    "    if col_filter_fun:\n",
    "        df = df.loc[:, [c for c in df.columns if col_filter_fun(c)]]\n",
    "    plot_donors_heatmap(\"[{}]: {}\".format(metric, tag), df, *args, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:21:01.405754Z",
     "start_time": "2017-10-13T16:21:01.397817Z"
    }
   },
   "outputs": [],
   "source": [
    "{k for k,v in signal_dfs_by_loci.keys() if not k.startswith(\"R\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All signal @ CGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T18:07:21.509226Z",
     "start_time": "2017-10-13T18:07:18.981778Z"
    }
   },
   "outputs": [],
   "source": [
    "for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "    plot_signal_heatmap(\"ucsc_cpgIslandExt.hg19\", norm, signal_dfs_by_loci, \n",
    "                        #col_filter_fun=lambda x: x == \"meth\",\n",
    "                        donors_difference=True, row_cluster=False, col_cluster=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All signal @ (DMR, 14_TssBiv, 15_Enh_Biv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:34:20.800893Z",
     "start_time": "2017-10-13T16:34:13.634002Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for loci in ['cd14_chromhmm.hg19.14_TssBiv', 'cd14_chromhmm.hg19.15_EnhBiv', \"washu_german_rrbs_filtered_dmrs_all_10.hg19\"]:\n",
    "    for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "        plot_signal_heatmap(loci, norm, signal_dfs_by_loci, \n",
    "                            col_filter_fun=lambda x: x == \"H3K4me1\",\n",
    "                            donors_difference=True, row_cluster=True, col_cluster=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3K3me1 signal @ loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T17:50:40.849628Z",
     "start_time": "2017-10-13T17:50:38.829578Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_signal_heatmap(\"H3K4me1\", \"rpkm\", signal_dfs_by_datatype, \n",
    "                    col_filter_fun=lambda loci: not loci.startswith(\"R-HSA\"),\n",
    "                    donors_difference=True, row_cluster=False, col_cluster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every data type @ ChromHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "    for histmod in {k for k,v in signal_dfs_by_datatype.keys()}:\n",
    "        plot_signal_heatmap(histmod, norm, signal_dfs_by_datatype, \n",
    "                            col_filter_fun=lambda loci: loci.startswith(\"ch14_chromhmm\"),\n",
    "                            donors_difference=True, row_cluster=False, col_cluster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T14:50:13.686754Z",
     "start_time": "2017-10-16T14:46:23.463334Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "signal_pvalues = defaultdict(list)\n",
    "missed_files = []\n",
    "ha = \"two-sided\" # 'less', 'two-sided', or 'greater'\n",
    "data_type_paths = [p for p in signal_root.iterdir() if p.is_dir()]\n",
    "for i, data_type_path in enumerate(data_type_paths, 1):\n",
    "    data_type = data_type_path.name\n",
    "    print(\"\\n[{}/{}] Processing: {}\".format(i, len(data_type_paths), data_type))\n",
    "    \n",
    "    for j, loci_path in enumerate(p for p in data_type_path.iterdir() if p.is_dir()):\n",
    "        loci = loci_path.name\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "        pvalues = {}\n",
    "        signal_normalizations = [\"raw\", \"rpkm\", \"rpm\"]\n",
    "        for norm in signal_normalizations:\n",
    "            files = [p for p in loci_path.glob(\"**/*_{}_data.csv\".format(norm))]\n",
    "            \n",
    "            assert len(files) <= 1, \"{}@{} [{}] Expected one file, but was {}: {}\".format(\n",
    "                data_type, loci, norm, len(files), files\n",
    "            )\n",
    "            if not len(files):\n",
    "                missed_files.append(\"{}@{} [{}]\".format(data_type, loci, norm))\n",
    "                continue\n",
    "            \n",
    "            df = pd.DataFrame.from_csv(files[0] , header=None)\n",
    "            df_ods = df.loc[[d for d in df.index if d.startswith(\"o\")],:]\n",
    "            df_yds = df.loc[[d for d in df.index if d.startswith(\"y\")],:]\n",
    "            pvalue = mannwhitneyu(df_ods.iloc[:,0], df_yds.iloc[:,0],\n",
    "                                  alternative=ha).pvalue\n",
    "            pvalues[norm] = pvalue\n",
    "\n",
    "        signal_pvalues[\"name\"].append(\"{}@{}\".format(data_type, loci))    \n",
    "        for norm in signal_normalizations:\n",
    "            signal_pvalues[norm].append(pvalues.get(norm, 1.0))\n",
    "        if j > 50:\n",
    "            break\n",
    "            \n",
    "print(\"Missed files: \", len(missed_files))\n",
    "print(\"  first 10:\", *missed_files[0:10])\n",
    "df = pd.DataFrame.from_dict(signal_pvalues)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T14:28:20.964061Z",
     "start_time": "2017-10-16T14:28:20.946406Z"
    }
   },
   "outputs": [],
   "source": [
    "# see: http://www.statsmodels.org/dev/_modules/statsmodels/stats/multitest.html\n",
    "df_fdr_bh = df.copy()\n",
    "for c in (c for c in df.columns if c != \"name\"):\n",
    "    _reject, pvals_corrected, *_ = multipletests(pvals=df.loc[:, c], \n",
    "                                                 alpha=0.05, method=\"fdr_bh\")\n",
    "    df_fdr_bh[c] = pvals_corrected\n",
    "    \n",
    "df_fdr_bh[\"min\"] = df_fdr_bh.min(axis=1)\n",
    "df_fdr_bh_sorted = df_fdr_bh.sort_values(by=\"min\")\n",
    "df_fdr_bh_005 = df_fdr_bh_sorted[df_fdr_bh_sorted[\"min\"] < 0.05]\n",
    "print(\"Passing FDR 0.05 by any metric:\", len(df_fdr_bh_005))\n",
    "df_fdr_bh_005.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "heatmaps\n",
    "\n",
    "* Have:\n",
    "    * loci vs loci\n",
    "    * loci vs ChromHMM\n",
    "    * Hist.mod consensus vs loci,chromHMM\n",
    "    * Peaks in hist.mod every donor (OD*, YD*) vs loci,chromHMM\n",
    "    * consensus vs consensus\n",
    "    * chipseq diff (Y-O) vs loci,chromHMM\n",
    "    * ([Y/O]x[Hist mod] consensus) vs loci,chromHMM \n",
    "\n",
    "* Todo:\n",
    "  * raw i-th donor(OD*, YD*) coverage vs loci,chromHMM\n",
    "  * \n",
    "  * \n",
    "  * \n",
    "  * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "todo2:\n",
    "\n",
    "4. why tf pipeline fails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
