{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:01:59.469862Z",
     "start_time": "2017-10-16T21:01:57.788127Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import subprocess \n",
    "import os\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from IPython.display import display\n",
    "import pybedtools\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:02:03.160759Z",
     "start_time": "2017-10-16T21:01:59.486538Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results_dir = Path(\"/mnt/stripe/bio/experiments/aging/loci_of_interest.tables\")\n",
    "# sorted_root = Path(\"/mnt/stripe/bio/experiments/aging/loci.sorted\")\n",
    "# THREADS_N = 32\n",
    "\n",
    "results_dir = Path(\"/Volumes/BigData/bio/experiments/aging/loci_of_interest.tables\")\n",
    "sorted_root = Path(\"/Volumes/BigData/bio/experiments/aging/loci.sorted\")\n",
    "THREADS_N = 8\n",
    "\n",
    "results_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:02:09.789843Z",
     "start_time": "2017-10-16T21:02:09.786138Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pybedtools.set_tempdir(\"/tmp\")\n",
    "pybedtools.cleanup()\n",
    "# !rm -rf {sorted_root}\n",
    "# !rm -rf {results_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:02:13.506801Z",
     "start_time": "2017-10-16T21:02:13.498364Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loci_root = Path(\"/mnt/stripe/bio/raw-data/aging/loci_of_interest\")\n",
    "# golden_peaks_root = Path(\"/mnt/stripe/bio/experiments/aging/peak_calling\")\n",
    "# zinbra_peaks_root = Path(\"/mnt/stripe/bio/experiments/configs/Y20O20/peaks\")\n",
    "\n",
    "loci_root = Path(\"/Volumes/BigData/bio/raw-data/aging/loci_of_interest\")\n",
    "golden_peaks_root = Path(\"/Volumes/BigData/bio/experiments/aging/peak_calling\") # *.*Peak\n",
    "zinbra_peaks_root = Path(\"/Volumes/BigData/bio/experiments/configs/Y20O20/peaks\") # *.bed\n",
    "\n",
    "signal_root = Path(\"/mnt/stripe/bio/experiments/signal\")\n",
    "\n",
    "chromhmm_root = loci_root / \"chromhmm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:02:42.635102Z",
     "start_time": "2017-10-16T21:02:42.065268Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls {loci_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:02:42.845810Z",
     "start_time": "2017-10-16T21:02:42.651129Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls {zinbra_peaks_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:02:42.991104Z",
     "start_time": "2017-10-16T21:02:42.863554Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls {golden_peaks_root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:00:01.940489Z",
     "start_time": "2017-10-16T23:00:01.614406Z"
    }
   },
   "outputs": [],
   "source": [
    "chromhmm_paths = list(chromhmm_root.glob('*.bed'))\n",
    "chromhmm_paths.sort(key=lambda p: int(p.name.split(\".\")[2].split(\"_\")[0]))\n",
    "\n",
    "CHROMHMM_ST_MAP = {\n",
    "    \"1_TssA\": \"Active TSS\",\n",
    "    \"2_TssFlnk\": \"Flanking TSS\",\n",
    "    \"3_TssFlnkU\": \"Flanking TSS Upstream\",\n",
    "    \"4_TssFlnkD\": \"Flanking TSS Downstream\",\n",
    "    \"5_Tx\": \"Strong transcription\",\n",
    "    \"6_TxWk\": \"Weak transcription\",\n",
    "    \"7_EnhG1\": \"Genic enhancer1\",\n",
    "    \"8_EnhG2\": \"Genic enhancer2\",\n",
    "    \"9_EnhA1\": \"Active Enhancer 1\",\n",
    "    \"10_EnhA2\": \"Active Enhancer 2\",\n",
    "    \"11_EnhWk\": \"Weak Enhancer\",\n",
    "    \"12_ZNF_Rpts\": \"ZNF genes & repeats\",\n",
    "    \"13_Het\": \"Heterochromatin\",\n",
    "    \"14_TssBiv\": \"Bivalent/Poised TSS\",\n",
    "    \"15_EnhBiv\": \"Bivalent Enhancer\",\n",
    "    \"16_ReprPC\": \"Repressed PolyComb\",\n",
    "    \"17_ReprPCWk\": \"Weak Repressed PolyComb\",\n",
    "    \"18_Quies\": \"Quiescent/Low\",\n",
    "}\n",
    "\n",
    "def chromhmm_state_descr(s):\n",
    "    chunks = s.split(\".\")\n",
    "    if len(chunks) > 2:\n",
    "        state = chunks[2]\n",
    "        if state in CHROMHMM_ST_MAP:\n",
    "            return \"{} ({})\".format(CHROMHMM_ST_MAP.get(chunks[2]), chunks[2])\n",
    "    return s\n",
    "\n",
    "for i, p in enumerate(chromhmm_paths):\n",
    "    print(chromhmm_state_descr(p.name), \"->\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot include all files from dir, because list is too big and heatmap becomes unreadable. Let's keep curated list\n",
    "of loci by rules:\n",
    "* root folder top level *.bed files\n",
    "* subfoldes: \"enchancers\", \"tfs\", \"regulatory\", \"weak_consensus\", \"zinbra_consensus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T22:29:12.019117Z",
     "start_time": "2017-10-16T22:29:11.817350Z"
    }
   },
   "outputs": [],
   "source": [
    "loci_paths = [p for p in loci_root.glob('*.bed')]\n",
    "for folder in [\"enchancers\", \"tfs\", \"regulatory\", \"repeats\"]:\n",
    "    loci_paths.extend([p for p in (loci_root / folder).glob('**/*.bed')])\n",
    "for folder in [\"golden_consensus\", \"zinbra_consensus\"]:\n",
    "    loci_paths.extend([p for p in (loci_root / folder).glob('*.bed') if (\"OD\" not in p.name) and (\"YD\" not in p.name)])\n",
    "loci_paths = sorted(loci_paths)\n",
    "loci_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:05:28.861648Z",
     "start_time": "2017-10-16T21:05:28.827928Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def donor_order_id(path):\n",
    "    chunks = path.name.split('_')\n",
    "    cands = list(filter(lambda s: len(s) > 2 and (s.startswith(\"OD\") or s.startswith(\"YD\")), chunks))\n",
    "    if len(cands) > 0:\n",
    "        donor_id = cands[0]\n",
    "        if donor_id[2] != \"S\":\n",
    "            return (donor_id[:2], int(donor_id[2:]))\n",
    "\n",
    "    return (path.name, 0)\n",
    "    \n",
    "\n",
    "def collect_peaks(peaks_roots):\n",
    "    result = {}\n",
    "    for peaks_root in [x for x in peaks_roots.iterdir() if x.is_dir() and x.name.startswith(\"H\")]:\n",
    "        print(\"Peaks:\", peaks_root)\n",
    "\n",
    "        peaks = list(chain(peaks_root.glob(\"**/*_peaks.bed\"),\n",
    "                           peaks_root.glob(\"**/bed/*-island.bed\"),\n",
    "                           peaks_root.glob(\"**/bed/*.*Peak\")\n",
    "                           # peaks_root.glob(\"**/*consensus*.bed\")  # Ignore\n",
    "                          ))\n",
    "        for p in peaks:\n",
    "            assert \"outlier\" not in str(p)\n",
    "        # e.g. \n",
    "        # * OD_OD14_H3K27ac_hg19_1.0E-6_peaks.bed\n",
    "        # * OD8_k27ac_hg19_broad_peaks.broadPeak\n",
    "        # * Ignore consensus: e.g. zinbra_weak_consensus.bed\n",
    "        peaks.sort(key=donor_order_id)\n",
    "        print(len(peaks))    \n",
    "        print(*[str(p) for p in peaks], sep=\"\\n\")\n",
    "        result[peaks_root.name] = peaks\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:09.147457Z",
     "start_time": "2017-10-16T21:34:08.813038Z"
    }
   },
   "outputs": [],
   "source": [
    "golden_peaks_by_histmod = collect_peaks(golden_peaks_root)\n",
    "golden_peaks_by_histmod.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:09.191530Z",
     "start_time": "2017-10-16T21:34:09.168004Z"
    }
   },
   "outputs": [],
   "source": [
    "zinbra_peaks_by_histmod = collect_peaks(zinbra_peaks_root)\n",
    "zinbra_peaks_by_histmod.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:10.455672Z",
     "start_time": "2017-10-16T21:34:10.426787Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zinbra_conensus_paths = [p for p in (loci_root / \"zinbra_consensus\").glob('*.bed')]\n",
    "zinbra_conensus_paths\n",
    "\n",
    "# Alternative:\n",
    "# zinbra_peaks_by_histmod = collect_peaks(zinbra_peaks_root)\n",
    "# consensus_peaks = []\n",
    "# for mod, peaks in zinbra_peaks_by_histmod.items():\n",
    "#     consensus_peaks.extend([p for p in peaks if \"consensus\" in p.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:10.677106Z",
     "start_time": "2017-10-16T21:34:10.663737Z"
    }
   },
   "outputs": [],
   "source": [
    "golden_conensus_paths = [p for p in (loci_root / \"golden_consensus\").glob('*.bed')]\n",
    "golden_conensus_paths\n",
    "\n",
    "# Alternative:\n",
    "# golden_peaks_by_histmod = collect_peaks(golden_peaks_root)\n",
    "# consensus_peaks = []\n",
    "# for mod, peaks in golden_peaks_by_histmod.items():\n",
    "#     consensus_peaks.extend([p for p in peaks if \"consensus\" in p.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:11.315945Z",
     "start_time": "2017-10-16T21:34:11.312798Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_loci = loci_paths + chromhmm_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:12.969363Z",
     "start_time": "2017-10-16T21:34:12.844079Z"
    }
   },
   "outputs": [],
   "source": [
    "!which bedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:13.485797Z",
     "start_time": "2017-10-16T21:34:13.463650Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# bedtrace.py\n",
    "def run(commands, stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE):\n",
    "    \"\"\"Launches pipe of commands given stdin and final stdout, stderr\"\"\"\n",
    "    processes = []\n",
    "    _stdin = stdin\n",
    "    \n",
    "    try:\n",
    "        for i, cmd in enumerate(commands):\n",
    "            if i < len(commands) - 1:\n",
    "                _stdout = subprocess.PIPE\n",
    "            else:\n",
    "                _stdout = stdout\n",
    "\n",
    "            p = subprocess.Popen(cmd, stdin=_stdin, stdout=_stdout,\n",
    "                                 stderr=stderr)\n",
    "            processes.append(p)\n",
    "            _stdin = p.stdout\n",
    "    finally:\n",
    "        for i in range(0, len(processes)):\n",
    "            try:\n",
    "                if i < len(processes) - 1:\n",
    "                    # Allow p1 to receive a SIGPIPE if p2 exits.\n",
    "                    processes[i].stdout.close()\n",
    "                else:\n",
    "                    return processes[i].communicate()\n",
    "            except Exception:\n",
    "                exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                print(\"Error running: {}\".format(commands))\n",
    "                # exc_type below is ignored on 3.5 and later\n",
    "                traceback.print_exception(exc_type, exc_value, exc_traceback,\n",
    "                                          #limit=2, \n",
    "                                          file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:16.285449Z",
     "start_time": "2017-10-16T21:34:16.229034Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "def as_sorted(p: Path, root: Path, sorted_root: Path):\n",
    "    sorted_p = sorted_root / p.relative_to(root)\n",
    "    sorted_p = sorted_p.parent / (sorted_p.stem + \".sorted.bed\")\n",
    "\n",
    "    if not sorted_p.exists():\n",
    "        sorted_p.parent.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Do not resort file if already sorted:\n",
    "        stderr = run(([\"sort\", \"-c\", \"-k1,1\", \"-k2,2n\", str(p)],))[1]\n",
    "        is_sorted = (len(stderr) == 0)\n",
    "        \n",
    "        if not is_sorted:\n",
    "            print(\"Sorting: \", str(p))\n",
    "            # By some reason BedTool.sort() fails to sort cds.csv\n",
    "            # bt.sort().saveas(sorted_p)\n",
    "            #stderr = run(([\"sort\", \"-c\", \"-k1,1\", \"-k2,2n\", str(sorted_p)],))[1]\n",
    "            #assert len(stderr) == 0, \"Expected to be sorted: {}\\nError:\\n{}\".format(sorted_p, stderr)\n",
    "            with open(str(sorted_p), \"w\") as f:\n",
    "                run(([\"sort\", \"-k1,1\", \"-k2,2n\", str(p)],), stdout=f)\n",
    "            print(\"  [Done]\", str(sorted_p))\n",
    "        else:   \n",
    "            # just copy file\n",
    "            shutil.copyfile(str(p), str(sorted_p))\n",
    "        \n",
    "        \n",
    "    return sorted_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:17.114164Z",
     "start_time": "2017-10-16T21:34:17.107104Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def as_sorted_bedtool(p: Path, root: Path, sorted_root: Path):\n",
    "#     sorted_p = sorted_root / p.relative_to(root)\n",
    "#     sorted_p = sorted_p.parent / (sorted_p.stem + \".sorted.bed\")\n",
    "\n",
    "#     if not sorted_p.exists():\n",
    "#         sorted_p.parent.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "#         # Do not resort file if already sorted:\n",
    "#         stderr = run(([\"sort\", \"-c\", \"-k1,1\", \"-k2,2n\", str(p)],))[1]\n",
    "#         is_sorted = (len(stderr) == 0)\n",
    "        \n",
    "#         bt = pybedtools.bedtool.BedTool(str(p))\n",
    "#         if not is_sorted:\n",
    "#             print(\"Sorting: \", str(p))\n",
    "#             # By some reason BedTool.sort() fails to sort cds.csv\n",
    "#             # bt.sort().saveas(sorted_p)\n",
    "#             #stderr = run(([\"sort\", \"-c\", \"-k1,1\", \"-k2,2n\", str(sorted_p)],))[1]\n",
    "#             #assert len(stderr) == 0, \"Expected to be sorted: {}\\nError:\\n{}\".format(sorted_p, stderr)\n",
    "#             with open(str(sorted_p), \"w\") as f:\n",
    "#                 run(([\"sort\", \"-k1,1\", \"-k2,2n\", str(p)],), stdout=f)\n",
    "#             print(\"  [Done]\", str(sorted_p))\n",
    "#         else:   \n",
    "#             # just copy file\n",
    "#             bt.saveas(str(sorted_p))\n",
    "#         del bt  # Too many open files issue\n",
    "        \n",
    "#     return pybedtools.bedtool.BedTool(str(sorted_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:50:26.888253Z",
     "start_time": "2017-10-16T21:50:26.798827Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, TimeoutError\n",
    "\n",
    "def run_bedtools_uniq_wc(ij, a, b):\n",
    "    output = run(([\"bedtools\", \"intersect\", \"-a\", str(a),\n",
    "                   \"-b\", str(b), \"-wa\"],\n",
    "                  [\"uniq\"], [\"wc\", \"-l\"]))\n",
    "    return (ij, int(output[0].decode().strip()))\n",
    "\n",
    "# def run_bedtools_uniq_wc(ij, a: pybedtools.BedTool, b: pybedtools.BedTool):\n",
    "#     # a = as_sorted_bedtool(a)\n",
    "#     # b = as_sorted_bedtool(b)\n",
    "#     c = a.intersect(b, wa=True)\n",
    "#     output = run(([\"cat\", c.fn], [\"uniq\"], [\"wc\", \"-l\"]))\n",
    "#     del c  # To many open files issues\n",
    "#     return (ij, int(output[0].decode().strip()))\n",
    "\n",
    "def run_bedtools_jaccard(ij, a, b):\n",
    "    output = run([(\"bash\", \"-c\", \"~/work/washu/bed/jaccard.sh '{}' '{}'\".format(str(a), str(b)))])\n",
    "    stdout = output[0].decode().strip()\n",
    "    return (ij, float(stdout))\n",
    "\n",
    "# def run_bedtools_jaccard(ij, a, b):\n",
    "#     #bed tools jaccard not symmetrix\n",
    "#     output = run(([\"bedtools\", \"jaccard\", \"-a\", str(a),\n",
    "#                    \"-b\", str(b)],\n",
    "#                   [\"cut\", \"-f\", \"3\"]))\n",
    "#     stdout = output[0].decode().strip()\n",
    "#     lines = stdout.split(\"\\n\")\n",
    "#     assert len(lines) == 2, lines\n",
    "#     assert lines[0] == \"jaccard\"\n",
    "#     return (ij, float(lines[1]))\n",
    "\n",
    "# def run_bedtools_jaccard(ij, a: pybedtools.BedTool, b: pybedtools.BedTool):\n",
    "#     # a = as_sorted_bedtool(a)\n",
    "#     # b = as_sorted_bedtool(b)\n",
    "#     return (ij, a.jaccard(b)[\"jaccard\"])\n",
    "\n",
    "def calc_intersection_table(a_paths, b_paths, path_to_sorted,\n",
    "                            threads=4, timeout_hours=10, jaccard=False):   \n",
    "    path_pairs = []\n",
    "    for i, a in enumerate(a_paths, 0):\n",
    "        for j, b in enumerate(b_paths, 1):\n",
    "            path_pairs.append(((i,j), path_to_sorted[a], path_to_sorted[b]))\n",
    "\n",
    "    metric = run_bedtools_jaccard if jaccard else run_bedtools_uniq_wc\n",
    "    pool = Pool(processes=threads) \n",
    "    multiple_results = [pool.apply_async(metric, \n",
    "                                         (ij, a, b)) for ij, a, b in path_pairs]\n",
    "    values = [res.get(timeout=3600*timeout_hours) for res in multiple_results]\n",
    "    \n",
    "    x = np.zeros((len(a_paths), 1 + len(b_paths)), np.float32)\n",
    "    for (i,j), value in values:\n",
    "        x[i, j] = value\n",
    "    \n",
    "    for i, a in enumerate(a_paths, 0):\n",
    "        output = run(([\"cat\", str(a)],[\"wc\", \"-l\"],))\n",
    "        x[i, 0] = int(output[0].decode().strip())\n",
    "               \n",
    "    df = pd.DataFrame(x,\n",
    "                      index=[f.name for f in a_paths],\n",
    "                      columns=[\"total\"] + [f.name for f in b_paths])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T14:16:43.004945Z",
     "start_time": "2017-10-17T14:16:42.910561Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import scipy.spatial as sp\n",
    "\n",
    "def plot_heatmap(title, df, path=None, autoscale=False, label_fun=None, figsize=(10,10),\n",
    "                 col_cluster=False, row_cluster=False,\n",
    "                 as_dist_matrix=False, \n",
    "                 row_donors=False):\n",
    "    if autoscale:\n",
    "        vmin, vmax = None, None\n",
    "    else:\n",
    "        vmin, vmax = 0, 1\n",
    "        \n",
    "    if label_fun:\n",
    "        df = df.copy()\n",
    "        df.columns = [label_fun(s) for s in df.columns]\n",
    "        df.index = [label_fun(s) for s in df.index]\n",
    "  \n",
    "    if row_donors:\n",
    "        donors_colors = []\n",
    "        for fname in df.index:\n",
    "            chunks = [ch.lower() for ch in fname.split(\"_\")]\n",
    "\n",
    "            if any(1 for ch in chunks if ch.startswith(\"od\")):\n",
    "                donors_colors.append(\"g\")\n",
    "            elif any(1 for ch in chunks if ch.startswith(\"yd\")):\n",
    "                donors_colors.append(\"b\")\n",
    "            else:\n",
    "                donors_colors.append(\"gray\")\n",
    "#         donors_colors = [\"g\" if d.lower().startswith(\"od\") else (\"b\" if d.lower().startswith(\"YD\") else \"b\")\n",
    "#                      for d in df.index]\n",
    "        row_colors = pd.Series(data=donors_colors, index=df.index, name=\"age\")\n",
    "    else:\n",
    "        row_colors = None\n",
    "\n",
    "#     if as_dist_matrix: #impl only for square matrix\n",
    "#         dissimilarity = sp.distance.squareform(1 - df)\n",
    "#         linkage = hierarchy.linkage(dissimilarity, method=\"average\")\n",
    "#         clusters = hierarchy.dendrogram(linkage, no_plot=True)['leaves']\n",
    "\n",
    "#         cg = sns.clustermap(df, \n",
    "#                             row_linkage=linkage, col_linkage=linkage,\n",
    "#                             col_cluster=col_cluster, row_cluster=row_cluster,\n",
    "#                             figsize=figsize, cmap=\"rainbow\",\n",
    "#                             vmin=vmin, vmax=vmax\n",
    "#                            )\n",
    "#         plt.setp(cg.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "#         plt.setp(cg.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
    "#     else:\n",
    "    g = sns.clustermap(df,\n",
    "                       col_cluster=col_cluster, row_cluster=row_cluster,\n",
    "                       figsize=figsize, cmap=\"rainbow\",\n",
    "                       metric=\"chebyshev\",\n",
    "                       vmin=vmin, vmax=vmax, robust=True, #robust=True: ignore color outliers\n",
    "                       row_colors=row_colors)\n",
    "    \n",
    "    plt.setp(g.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.title(title)\n",
    "    if path is None:        \n",
    "        plt.show()\n",
    "    else:\n",
    "        pp.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:18.776084Z",
     "start_time": "2017-10-16T21:34:18.763305Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_intersection_table(beds, loci, path_to_bt, result_path, threads=4, jaccard=False):\n",
    "    if result_path.exists():\n",
    "        df = pd.DataFrame.from_csv(result_path)\n",
    "        print(\"Loaded: \", result_path)\n",
    "    else:\n",
    "        print(\"Calculating: \", result_path)\n",
    "        df = calc_intersection_table(beds, loci, path_to_bt, threads=threads, jaccard=jaccard) \n",
    "        result_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(str(result_path))\n",
    "        print(\"  Saved: \", result_path)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:34:19.105556Z",
     "start_time": "2017-10-16T21:34:19.099683Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    return df.divide(df[\"total\"], axis=0).drop(\"total\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T21:55:52.177726Z",
     "start_time": "2017-10-17T21:55:52.088753Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_intersection(beds, loci, path_to_bt, results_dir, tag,\n",
    "                         figsize=(10,10), col_cluster=False, row_cluster=True,\n",
    "                         all_metrics=0,\n",
    "                         **kw):\n",
    "    dfs_n = []\n",
    "    df_bl = load_intersection_table(beds, loci, path_to_bt, \n",
    "                                    results_dir / \"{}_bl.csv\".format(tag), threads=THREADS_N)\n",
    "    display(df_bl.head(3))\n",
    "    \n",
    "    df_n_bl = normalize(df_bl)\n",
    "    display(df_n_bl.head(3))\n",
    "    dfs_n.append(df_n_bl)\n",
    "\n",
    "    if all_metrics > 0:\n",
    "        df_lb = load_intersection_table(loci, beds, path_to_bt,\n",
    "                                        results_dir / \"{}_lb.csv\".format(tag), threads=THREADS_N)\n",
    "        display(df_lb.head(3))\n",
    "\n",
    "        df_n_lb = normalize(df_lb).T\n",
    "        display(df_n_lb.head(3))\n",
    "        dfs_n.append(df_n_lb)\n",
    "\n",
    "    if all_metrics > 2:\n",
    "        df_jaccard = load_intersection_table(beds, loci, path_to_bt, \n",
    "                                             results_dir / \"{}_js.csv\".format(tag), threads=THREADS_N,\n",
    "                                             jaccard = True)\n",
    "        df_jaccard = df_jaccard.drop(\"total\", axis=1)\n",
    "        display(df_jaccard.head(3))\n",
    "        dfs_n.append(df_jaccard)\n",
    "    \n",
    "    def inner_chromhmm_or_donor(name):\n",
    "        if \"row_donors\" in kw:\n",
    "            chunks = name.split('_')\n",
    "            cands = list(filter(\n",
    "                lambda s: len(s) > 2 and (s.startswith(\"OD\") or s.startswith(\"YD\")) and s[2] != \"S\",\n",
    "                chunks))\n",
    "            \n",
    "            if len(cands) > 0:\n",
    "                return cands[0]\n",
    "            \n",
    "        return chromhmm_state_descr(name)\n",
    "    \n",
    "    plot_heatmap(\"Metrics: # intervals from row file intersecting any interval from column file\",\n",
    "                 df_n_bl, autoscale=False, label_fun=inner_chromhmm_or_donor, figsize=figsize,\n",
    "                 col_cluster=col_cluster, row_cluster=row_cluster, **kw)\n",
    "    \n",
    "    if all_metrics > 0:\n",
    "        plot_heatmap(\"Metrics: # intervals from col file intersecting any interval from row file\",\n",
    "                     df_n_lb, autoscale=False, label_fun=inner_chromhmm_or_donor, figsize=figsize,\n",
    "                     col_cluster=col_cluster, row_cluster=row_cluster, **kw)\n",
    "    \n",
    "    if all_metrics > 1:\n",
    "        plot_heatmap(\"Metrics: Geometric mean for intersectiong intervals\",\n",
    "                     np.sqrt(df_n_bl*df_n_lb), autoscale=False, label_fun=inner_chromhmm_or_donor, figsize=figsize,\n",
    "                     col_cluster=col_cluster, row_cluster=row_cluster, **kw)\n",
    "\n",
    "    if all_metrics > 2:\n",
    "        plot_heatmap(\"Metrics: Jaccard\",\n",
    "                     df_jaccard, autoscale=True, label_fun=inner_chromhmm_or_donor, figsize=figsize,\n",
    "                     col_cluster=col_cluster, row_cluster=row_cluster,\n",
    "                     as_dist_matrix=True, **kw)\n",
    "    return dfs_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T12:57:05.828030Z",
     "start_time": "2017-10-17T12:57:05.785207Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_interests_loci_folder(folder_name,):\n",
    "    paths = sorted([p for p in (loci_root / folder_name).glob('**/*.bed')])\n",
    "    suffix = folder_name\n",
    "    print(\"Paths: #\", len(paths))\n",
    "\n",
    "    # ChromHMM\n",
    "    print(\"Ensure files sorted...\")\n",
    "    mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in paths}\n",
    "    mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in chromhmm_paths})\n",
    "    print(\"[Done]\")\n",
    "    process_intersection(paths, chromhmm_paths, mapping, results_dir, \"{}_chromhmm\".format(suffix), figsize=(8,8),\n",
    "                         all_metrics=1)\n",
    "    \n",
    "    # Consensus: Y+O\n",
    "    consensus_paths = sorted([p for p in (zinbra_conensus_paths + golden_conensus_paths) if \"DS\" not in p.name],\n",
    "                             key=lambda p: p.name)\n",
    "    print(\"Ensure files sorted...\")\n",
    "    mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in paths}\n",
    "    mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in consensus_paths})\n",
    "    print(\"[Done]\")\n",
    "    process_intersection(paths, consensus_paths, mapping, results_dir, \"{}_consensus_short\".format(suffix),\n",
    "                         figsize=(8,8), row_cluster=True, col_cluster=False,\n",
    "                         all_metrics=1)\n",
    "\n",
    "    # Consensus: Y,O,Y+O\n",
    "    consensus_paths = sorted(zinbra_conensus_paths + golden_conensus_paths,\n",
    "                             key=lambda p: p.name)\n",
    "    print(\"Ensure files sorted...\")\n",
    "    mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in paths}\n",
    "    mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in consensus_paths})\n",
    "    print(\"[Done]\")\n",
    "    process_intersection(paths, consensus_paths, mapping, results_dir, \"{}_consensus\".format(suffix),\n",
    "                         figsize=(12,8), row_cluster=True, col_cluster=False)\n",
    "\n",
    "    # Loci\n",
    "    print(\"Ensure files sorted...\")\n",
    "    mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in paths}\n",
    "    mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in loci_paths})\n",
    "    print(\"[Done]\")\n",
    "    process_intersection(paths, loci_paths, mapping, results_dir, \"{}_loci\".format(suffix), figsize=(20,8),\n",
    "                         all_metrics=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:50:42.583532Z",
     "start_time": "2017-10-16T21:50:38.896494Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm {results_dir}/tmp0_loci.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T21:51:34.739951Z",
     "start_time": "2017-10-16T21:50:42.605439Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "tmp_loci_paths = loci_paths[0:6]\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in tmp_loci_paths}\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(tmp_loci_paths, tmp_loci_paths, mapping, results_dir, \"tmp0_loci.csv\", figsize=(5,5))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chose Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in loci_paths}\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(loci_paths, loci_paths, mapping, results_dir, \"loci\", figsize=(15,15),\n",
    "                     col_cluster=True,\n",
    "                     all_metrics=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T14:31:55.944678Z",
     "start_time": "2017-10-10T14:31:55.941977Z"
    }
   },
   "source": [
    "## Loci vs Loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T09:46:14.396321Z",
     "start_time": "2017-10-12T09:25:33.123699Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in loci_paths}\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(loci_paths, loci_paths, mapping, results_dir, \"loci\", figsize=(15,15),\n",
    "                     col_cluster=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loci vs ChromHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T09:19:08.431930Z",
     "start_time": "2017-10-12T09:19:08.407935Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in loci_paths}\n",
    "mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in chromhmm_paths})\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(loci_paths, chromhmm_paths, mapping, results_dir, \"loci_chromhmm\", figsize=(8, 15))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T14:32:34.287847Z",
     "start_time": "2017-10-10T14:32:34.284244Z"
    }
   },
   "source": [
    "## ChromHMM vs ChromHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:19:47.588477Z",
     "start_time": "2017-10-11T12:19:45.275497Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in chromhmm_paths}\n",
    "print(\"[Done]\")\n",
    "process_intersection(chromhmm_paths, chromhmm_paths, mapping, results_dir, \"chromhmm\", figsize=(8,8))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Consensus vs Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T15:44:08.691366Z",
     "start_time": "2017-10-17T15:43:58.488088Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "consensus_paths = sorted([p for p in (zinbra_conensus_paths + golden_conensus_paths) if \"DS\" not in p.name],\n",
    "                         key=lambda p: p.name)\n",
    "for p in (consensus_paths):\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "dfs = process_intersection(consensus_paths, consensus_paths, mapping, results_dir, \"consensus_short\", figsize=(6,6),\n",
    "                     col_cluster=True)\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T22:54:41.051754Z",
     "start_time": "2017-10-16T22:54:34.664154Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "consensus_paths = sorted(zinbra_conensus_paths + golden_conensus_paths,\n",
    "                         key=lambda p: p.name)\n",
    "for p in (consensus_paths):\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(consensus_paths, consensus_paths, mapping, results_dir, \"consensus\", figsize=(15,8),\n",
    "                     col_cluster=False)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zinbra vs Loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:21:35.023637Z",
     "start_time": "2017-10-11T12:21:25.273821Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for p in zinbra_conensus_paths:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "for p in all_loci:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(zinbra_conensus_paths, all_loci, mapping, results_dir, \"zinbra_consensus_vs_loci\", \n",
    "                     figsize=(20,4), row_donors=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Hist mods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:20:55.221079Z",
     "start_time": "2017-10-11T12:20:45.915527Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for mod, peaks in zinbra_peaks_by_histmod.items():\n",
    "    for p in peaks:\n",
    "        mapping[p] = as_sorted(p, zinbra_peaks_root, sorted_root / \"zinbra\")\n",
    "for p in all_loci:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "for mod, peaks in zinbra_peaks_by_histmod.items():\n",
    "    print(mod)\n",
    "    process_intersection(peaks, all_loci, mapping, results_dir, \"zinbra_{}_vs_loci\".format(mod),\n",
    "                         figsize=(20,10), col_cluster=True, row_cluster=True)\n",
    "None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macs vs Loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conensus peaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for p in golden_conensus_paths:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root  / \"loci_of_interest\")\n",
    "for p in all_loci:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "process_intersection(golden_conensus_paths, all_loci, mapping, results_dir, \"golden_consensus_vs_loci\",\n",
    "                     figsize=(20,4), row_donors=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Hist mods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:23:09.460510Z",
     "start_time": "2017-10-11T12:22:57.250421Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Ensure files sorted...\")\n",
    "mapping = {}\n",
    "for mod, peaks in golden_peaks_by_histmod.items():\n",
    "    for p in peaks:\n",
    "        mapping[p] = as_sorted(p, golden_peaks_root, sorted_root / \"golden\")\n",
    "for p in all_loci:\n",
    "    mapping[p] = as_sorted(p, loci_root, sorted_root / \"loci_of_interest\")\n",
    "print(\"[Done]\")\n",
    "\n",
    "for mod, peaks in golden_peaks_by_histmod.items():\n",
    "    print(mod)\n",
    "    process_intersection(peaks, all_loci, mapping, results_dir, \"golden_{}_vs_loci\".format(mod),\n",
    "                         figsize=(20,10), row_donors=True,\n",
    "                         col_cluster=True, row_cluster=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check_interests_loci_folder(\"pathway\")\n",
    "# to many data to calc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T10:43:02.444028Z",
     "start_time": "2017-10-17T10:43:02.408389Z"
    }
   },
   "outputs": [],
   "source": [
    "check_interests_loci_folder(\"repeats\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T12:30:40.029391Z",
     "start_time": "2017-10-17T12:30:40.021480Z"
    }
   },
   "source": [
    "##  ChipSeq Diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vc ChromHmm, Loci, Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T12:31:47.613837Z",
     "start_time": "2017-10-17T12:31:47.607904Z"
    }
   },
   "outputs": [],
   "source": [
    "check_interests_loci_folder(\"chipseq_diff_loci\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###  vs ChromHMM\n",
    "\n",
    "# diff_chip_root = loci_root / \"chipseq_diff_loci\"\n",
    "# diff_chip_paths = [p for p in diff_chip_root.glob('**/*.bed')]\n",
    "\n",
    "# print(\"Ensure files sorted...\")\n",
    "# mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in diff_chip_paths}\n",
    "# mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in chromhmm_paths})\n",
    "# print(\"[Done]\")\n",
    "# process_intersection(diff_chip_paths, chromhmm_paths, mapping, results_dir, \"diff_chip_chromhmm\", figsize=(8,8))\n",
    "\n",
    "### vs Loci\n",
    "# diff_chip_root = loci_root / \"chipseq_diff_loci\"\n",
    "# diff_chip_paths = [p for p in diff_chip_root.glob('**/*.bed')]\n",
    "\n",
    "# print(\"Ensure files sorted...\")\n",
    "# mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in diff_chip_paths}\n",
    "# mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in loci_paths})\n",
    "# print(\"[Done]\")\n",
    "# process_intersection(diff_chip_paths, loci_paths, mapping, results_dir, \"diff_chip_loci\", figsize=(8,8))\n",
    "\n",
    "\n",
    "### vs Consensus\n",
    "# diff_chip_root = loci_root / \"chipseq_diff_loci\"\n",
    "# diff_chip_paths = [p for p in diff_chip_root.glob('**/*.bed')]\n",
    "\n",
    "# consensus_paths = zinbra_conensus_paths + golden_conensus_paths\n",
    "# print(\"Ensure files sorted...\")\n",
    "# mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in diff_chip_paths}\n",
    "# mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in consensus_paths})\n",
    "# print(\"[Done]\")\n",
    "# process_intersection(diff_chip_paths, consensus_paths, mapping, results_dir, \"diff_chip_consensus\",\n",
    "#                      figsize=(8,8), row_cluster=False, col_cluster=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vs Repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T12:30:24.550097Z",
     "start_time": "2017-10-17T12:30:20.375816Z"
    }
   },
   "outputs": [],
   "source": [
    "diff_chip_paths = [p for p in (loci_root / \"chipseq_diff_loci\").glob('**/*.bed')]\n",
    "repeats_paths = [p for p in (loci_root / \"repeats\").glob('**/*.bed')]\n",
    "\n",
    "print(\"Ensure files sorted...\")\n",
    "mapping = {p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in diff_chip_paths}\n",
    "mapping.update({p:as_sorted(p, loci_root, sorted_root / \"loci_of_interest\") for p in repeats_paths})\n",
    "print(\"[Done]\")\n",
    "process_intersection(diff_chip_paths, repeats_paths, mapping, results_dir, \"chipseq_diff_repeats\",\n",
    "                     figsize=(8,8), row_cluster=False, col_cluster=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RnaSeq diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_interests_loci_folder(\"rna_diff\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage (Signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T19:33:58.337849Z",
     "start_time": "2017-10-17T19:33:58.329247Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T16:04:28.379693Z",
     "start_time": "2017-10-17T16:04:28.372945Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signals_results_dir = results_dir / \"signals\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm -rf {signals_results_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T16:42:08.136966Z",
     "start_time": "2017-10-17T16:42:03.321561Z"
    }
   },
   "outputs": [],
   "source": [
    "if signals_results_dir.exists():\n",
    "    # TODO: load\n",
    "    pass\n",
    "else:\n",
    "    signal_dfs_by_datatype = {}\n",
    "    signal_dfs_by_loci = {}\n",
    "    missed_files = []\n",
    "\n",
    "    signals_results_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    series_by_loci = defaultdict(list)\n",
    "    data_type_paths = [p for p in signal_root.iterdir() if p.is_dir()]\n",
    "    for i, data_type_path in enumerate(data_type_paths, 1):\n",
    "        data_type = data_type_path.name\n",
    "        print(\"[{}/{}] Processing: {}\".format(i, len(data_type_paths), data_type))\n",
    "\n",
    "        for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "            print(\"  Normalization:\", norm)\n",
    "            series_by_datatype = []\n",
    "\n",
    "            # TODO: load from results dir?\n",
    "            for loci_path in (p for p in data_type_path.iterdir() if p.is_dir()):\n",
    "                loci = loci_path.name\n",
    "                files = [p for p in loci_path.glob(\"**/*_{}_data.csv\".format(norm))]\n",
    "\n",
    "                assert len(files) <= 1, \"{}@{} [{}] Expected one file, but was {}: {}\".format(\n",
    "                    data_type, loci, norm, len(files), files\n",
    "                )\n",
    "                if not len(files):\n",
    "                    missed_files.append(\"{}@{} [{}]\".format(data_type, loci, norm))\n",
    "                    continue\n",
    "\n",
    "                df = pd.DataFrame.from_csv(files[0] , header=None)\n",
    "                series = df.iloc[:,0]\n",
    "                series.name = loci\n",
    "\n",
    "                series_by_datatype.append(series) \n",
    "\n",
    "                series2 = series.copy()\n",
    "                series2.name = data_type\n",
    "                series_by_loci[(loci, norm)].append(series2)\n",
    "\n",
    "            if len(series_by_datatype):\n",
    "                # by data type:    \n",
    "                df = pd.DataFrame(series_by_datatype, )\n",
    "                #df.index = [f.stem for f in itertools.islice(files, 10)]\n",
    "\n",
    "                df.to_csv(str(signals_results_dir / \"signal_{}_{}.csv\".format(data_type, norm)))\n",
    "            else:\n",
    "                df = None\n",
    "            signal_dfs_by_datatype[(data_type, norm)] = df\n",
    "\n",
    "    for (loci, norm), series in series_by_loci.items():\n",
    "        if len(series):\n",
    "            df = pd.DataFrame(series, )\n",
    "            df.to_csv(str(signals_results_dir / \"signal_{}_{}\".format(loci, norm)))\n",
    "        else:\n",
    "            df = None\n",
    "        signal_dfs_by_loci[(loci, norm)] = df\n",
    "        \n",
    "    print(\"Missed files: \", len(missed_files))\n",
    "    print(\"  first 10:\", *missed_files[0:10])\n",
    "    print(\"Signal by datatype, missed records:\", str(sum(1 for v in signal_dfs_by_datatype.values() if v is None)))\n",
    "    print(\"  \", [k for k,v in signal_dfs_by_datatype.items() if v is None])\n",
    "    print(\"Signal by loci, missed records:\", str(sum(1 for v in signal_dfs_by_loci.values() if v is None)))    \n",
    "    print(\"  \", [k for k,v in signal_dfs_by_loci.items() if v is None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:08:46.867082Z",
     "start_time": "2017-10-13T16:08:46.840964Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_dfs_by_datatype[(\"H3K4me1\", \"rpkm\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:09:54.202948Z",
     "start_time": "2017-10-13T16:09:54.177877Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_dfs_by_loci[(\"washu_german_rrbs_filtered_dmrs_all_10.hg19\", \"rpkm\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_donors_heatmap(title, df, path=None, autoscale=False, \n",
    "                        label_fun=None, figsize=(10,10),\n",
    "                        col_cluster=False, row_cluster=False):\n",
    "    if autoscale:\n",
    "        vmin, vmax = None, None\n",
    "    else:\n",
    "        vmin, vmax = 0, 1\n",
    "        \n",
    "    if label_fun:\n",
    "        df = df.copy()\n",
    "        df.columns = [label_fun(s) for s in df.columns]\n",
    "        df.index = [label_fun(s) for s in df.index]\n",
    "        \n",
    "    donors_colors = [\"g\" if d.lower().startswith(\"od\") else (\"b\" if d.lower().startswith(\"YD\") else \"b\")\n",
    "                     for d in df.index]\n",
    "    row_colors = pd.Series(data=donors_colors, index=df.index, name=\"age\")\n",
    "            \n",
    "    g = sns.clustermap(df,\n",
    "                       col_cluster=col_cluster, row_cluster=row_cluster,\n",
    "                       figsize=figsize, cmap=\"rainbow\",\n",
    "                       metric=\"chebyshev\",\n",
    "                       standard_scale = None,\n",
    "                       vmin=vmin, vmax=vmax,\n",
    "                       row_colors=row_colors,\n",
    "                       robust=True) #robust=True: ignore color outliers\n",
    "    plt.setp(g.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "\n",
    "    plt.title(title)\n",
    "    if path is None:        \n",
    "        plt.show()\n",
    "    else:\n",
    "        pp.savefig()\n",
    "        \n",
    "def plot_signal_heatmap(tag, metric, signal_dfs, *args,\n",
    "                        col_filter_fun=None,\n",
    "                        **kw):\n",
    "\n",
    "    df = signal_dfs[(tag, metric)]\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    df = df.T\n",
    "        \n",
    "    # let's sort by index, not just lexicographically, but in human readable order, e.g. OD2 shoud be before OD10\n",
    "    def inner_donor_order_id(name):\n",
    "        assert (len(name) > 2 and (name.startswith(\"od\") or name.startswith(\"yd\")))\n",
    "        return (name[:2], int(name[2:]))\n",
    "\n",
    "    df = df.loc[sorted(df.index.tolist(), key=inner_donor_order_id), :]\n",
    "    \n",
    "    if col_filter_fun:\n",
    "        df = df.loc[:, [c for c in df.columns if col_filter_fun(c)]]\n",
    "\n",
    "    # Normalize by columns (by loci across all donors)\n",
    "    df = ((df - np.min(df, axis=0))/(np.max(df, axis=0) - np.min(df, axis=0)))\n",
    "    plot_donors_heatmap(\"[{}]: {}\".format(metric, tag), df, *args, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:21:01.405754Z",
     "start_time": "2017-10-13T16:21:01.397817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not pathways:{k for k,v in signal_dfs_by_loci.keys() if not k.startswith(\"R\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All signal @ CGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T18:07:21.509226Z",
     "start_time": "2017-10-13T18:07:18.981778Z"
    }
   },
   "outputs": [],
   "source": [
    "for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "    plot_signal_heatmap(\"ucsc_cpgIslandExt.hg19\", norm, signal_dfs_by_loci, \n",
    "                        #col_filter_fun=lambda x: x == \"meth\",\n",
    "                        row_cluster=False, col_cluster=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All signal @ (DMR, 14_TssBiv, 15_Enh_Biv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T16:34:20.800893Z",
     "start_time": "2017-10-13T16:34:13.634002Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for loci in ['cd14_chromhmm.hg19.14_TssBiv', 'cd14_chromhmm.hg19.15_EnhBiv', \"washu_german_rrbs_filtered_dmrs_all_10.hg19\"]:\n",
    "    for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "        plot_signal_heatmap(loci, norm, signal_dfs_by_loci, \n",
    "                            col_filter_fun=lambda x: x == \"H3K4me1\",\n",
    "                            row_cluster=True, col_cluster=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3K3me1 signal @ loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:55:05.068956Z",
     "start_time": "2017-10-17T17:55:05.042283Z"
    }
   },
   "outputs": [],
   "source": [
    "for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "    plot_signal_heatmap(\"H3K4me1\", norm, signal_dfs_by_datatype, \n",
    "                        col_filter_fun=lambda loci: not loci.startswith(\"R-HSA\"),\n",
    "                        row_cluster=False, col_cluster=True,\n",
    "                        figsize=(24, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every data type @ ChromHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for norm in [\"raw\", \"rpkm\", \"rpm\"]:\n",
    "    for histmod in {k for k,v in signal_dfs_by_datatype.keys()}:\n",
    "        display(norm + \":\" + histmod)\n",
    "        plot_signal_heatmap(histmod, norm, signal_dfs_by_datatype, \n",
    "                            col_filter_fun=lambda loci: loci.startswith(\"cd14_chromhmm\"),\n",
    "                            row_cluster=False, col_cluster=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T07:50:08.446485Z",
     "start_time": "2017-10-17T07:50:08.419959Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T14:50:13.686754Z",
     "start_time": "2017-10-16T14:46:23.463334Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "signal_pvalues = defaultdict(list)\n",
    "missed_files = []\n",
    "problem_files = []\n",
    "ha = \"two-sided\" # 'less', 'two-sided', or 'greater'\n",
    "data_type_paths = [p for p in signal_root.iterdir() if p.is_dir()]\n",
    "for i, data_type_path in enumerate(data_type_paths, 1):\n",
    "    data_type = data_type_path.name\n",
    "    print(\"\\n[{}/{}] Processing: {}\".format(i, len(data_type_paths), data_type))\n",
    "    \n",
    "    for j, loci_path in enumerate(p for p in data_type_path.iterdir() if p.is_dir()):\n",
    "        loci = loci_path.name\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "        pvalues = {}\n",
    "        signal_normalizations = [\"raw\", \"rpkm\", \"rpm\"]\n",
    "        for norm in signal_normalizations:\n",
    "            files = [p for p in loci_path.glob(\"**/*_{}_data.csv\".format(norm))]\n",
    "            \n",
    "            assert len(files) <= 1, \"{}@{} [{}] Expected one file, but was {}: {}\".format(\n",
    "                data_type, loci, norm, len(files), files\n",
    "            )\n",
    "            if not len(files):\n",
    "                missed_files.append(\"{}@{} [{}]\".format(data_type, loci, norm))\n",
    "                continue\n",
    "            \n",
    "            file = files[0]\n",
    "            df = pd.DataFrame.from_csv(file , header=None)\n",
    "            df_ods = df.loc[[d for d in df.index if d.startswith(\"o\")],:]\n",
    "            df_yds = df.loc[[d for d in df.index if d.startswith(\"y\")],:]\n",
    "            try:\n",
    "                pvalue = mannwhitneyu(df_ods.iloc[:,0], df_yds.iloc[:,0],\n",
    "                                      alternative=ha).pvalue\n",
    "            except ValueError as e:\n",
    "                print(\"Error: {} in file:\\n{}\".format(e, file))\n",
    "                problem_files.append(file)    \n",
    "            pvalues[norm] = pvalue\n",
    "\n",
    "        signal_pvalues[\"name\"].append(\"{}@{}\".format(data_type, loci))    \n",
    "        for norm in signal_normalizations:\n",
    "            signal_pvalues[norm].append(pvalues.get(norm, np.nan)) #1.0\n",
    "\n",
    "print(\"Missed files: \", len(missed_files))\n",
    "print(\"  first 10:\", *missed_files[0:10])\n",
    "print(\"Errors occurred in files: \", len(problem_files))\n",
    "print(\"  first 10:\", *problem_files[0:10])\n",
    "\n",
    "signal_pvalues_df = pd.DataFrame.from_dict(signal_pvalues)\n",
    "signal_pvalues_df.to_csv(str(results_dir / \"signal_pvalues.csv\"))\n",
    "signal_pvalues_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal_pvalues_df.index = signal_pvalues_df.name\n",
    "signal_pvalues_df.drop(\"name\", inplace=True, axis=1)\n",
    "\n",
    "print(\"Not corrected pval, first 10 lowerest pvalues:\")\n",
    "signal_pvalues_df[\"min\"] = signal_pvalues_df.min(axis=1)\n",
    "signal_pvalues_df.sort_values(by=\"min\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manhattan_plot(pvalues_df, correction=\"Uncorrected\"):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i, norm in enumerate([\"raw\", \"rpkm\", \"rpm\"], 1):\n",
    "        n = pvalues_df.shape[0]\n",
    "        ax = plt.subplot(2, 2, i)\n",
    "        #plt.plot(range(n), -np.log10(pvalues_df[\"raw\"]), marker=\".\", ls=\"\")\n",
    "        #ax.hlines(y=-np.log10(0.05), xmin=0, xmax=n, color=\"r\", linestyle='dotted')\n",
    "        ax.plot(range(n), 1/pvalues_df[norm], marker=\".\", ls=\"\")\n",
    "        ax.axhline(y=-np.log10(0.05), xmin=0, xmax=n, color=\"r\", linestyle='dotted')\n",
    "        ax.set_ylabel(\"{} pvalues (-log(p) scale )\".format(correction))\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_title(\"[{}] Mann whitney u test pvalues\".format(norm))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manhattan_plot(signal_pvalues_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T14:28:20.964061Z",
     "start_time": "2017-10-16T14:28:20.946406Z"
    }
   },
   "outputs": [],
   "source": [
    "# P-values correction\n",
    "#\n",
    "# see: http://www.statsmodels.org/dev/_modules/statsmodels/stats/multitest.html\n",
    "signal_pvalues_bh_df = signal_pvalues_df.copy().drop(\"min\", axis=1)\n",
    "for c in signal_pvalues_bh_df.columns:\n",
    "    pvals = signal_pvalues_bh_df.loc[:, c]\n",
    "    pvals_not_nan_mask = ~np.isnan(pvals)\n",
    "    pvals_not_nan = pvals[pvals_not_nan_mask]\n",
    "    _reject, pvals_corrected, *_ = multipletests(pvals=pvals_not_nan, \n",
    "                                                 alpha=0.05, method=\"fdr_bh\") #fdr_bh, holm-sidak, bonferroni \n",
    "    signal_pvalues_bh_df.loc[pvals_not_nan_mask, c] = pvals_corrected\n",
    "#     _reject, pvals_corrected, *_ = multipletests(pvals=signal_pvalues_df.loc[:, c], \n",
    "#                                                  alpha=0.05, method=\"fdr_bh\")\n",
    "#     df_fdr_bh[c] = pvals_corrected\n",
    "    \n",
    "signal_pvalues_bh_df[\"min\"] = signal_pvalues_bh_df.min(axis=1, skipna=True)\n",
    "signal_pvalues_bh_sorted_df = signal_pvalues_bh_df.sort_values(by=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manhattan_plot(signal_pvalues_bh_sorted_df, \"BenjaminiHochberg corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal_pvalues_bh_sorted_df_005 = signal_pvalues_bh_sorted_df[signal_pvalues_bh_sorted_df[\"min\"] < 0.05]\n",
    "print(\"Passing FDR 0.05 by any metric:\", len(signal_pvalues_bh_sorted_df_005))\n",
    "signal_pvalues_bh_sorted_df_005.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal_pvalues_df.loc[signal_pvalues_bh_sorted_df_005.head(10).index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Corrected, first 10 lowerest pvalues:\")\n",
    "display(signal_pvalues_bh_sorted_df.head(10))\n",
    "\n",
    "print(\"Same records, but original pvalues:\")\n",
    "display(signal_pvalues_df.loc[signal_pvalues_bh_sorted_df.head(10).index, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.min(signal_pvalues_bh_sorted_df[\"raw\"]), np.max(signal_pvalues_bh_sorted_df[\"raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Corrected, dmr related pvalues:\")\n",
    "signal_pvalues_bh_sorted_df.loc[signal_pvalues_bh_sorted_df.index.str.contains(\"dmr\"), :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "698px",
    "left": "0px",
    "right": "1124px",
    "top": "106px",
    "width": "268px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
